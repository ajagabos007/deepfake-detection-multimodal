# Roadmap - Deepfake Detection (Multimodal)

## 1. Setup & Environment
- [x] Create virtual environment  
- [x] Install dependencies (`numpy`, `opencv`, `librosa`, `torch`, etc.)  
- [x] Setup `requirements.txt` (CPU-only friendly)  

---

## 2. Data Preprocessing
- [x] `video.py` â†’ extract frames, detect faces  
- [x] `audio.py` â†’ extract & preprocess audio (MFCCs etc.)  
- [ ] `context.py` â†’ transcripts (via Whisper, subtitles parsing)  

---

## 3. Feature Extraction
- [x] `extract_visual.py` â†’ convert video frames â†’ embeddings  
- [x] `extract_audio.py` â†’ convert audio â†’ features  
- [ ] `extract_context.py` â†’ convert transcripts â†’ embeddings  

---

## 4. Model Design
- [x] `visual_net.py` â†’ CNN / ResNet for video  
- [x] `audio_net.py` â†’ LSTM / CNN for audio  
- [x] `context_net.py` â†’ Transformer/LSTM for text  
- [x] `fusion_net.py` â†’ late fusion multimodal network (basic draft)  

---

## 5. Training
- [*] `train_visual.py`  
- [*] `train_audio.py`  
- [*] `train_context.py`  
- [*] `train_fusion.py` (skeleton exists, needs full loop)  

---

## 6. Prediction Pipeline
- [ ] `predict.py` â†’ run preprocessing + feature extraction + fusion model â†’ output *real/fake*  

---

## 7. Evaluation
- [ ] Metrics (accuracy, F1, AUC)  
- [ ] Confusion matrix  
- [ ] Compare single-modal vs multimodal performance  

---

## 8. Interface
- [ ] Simple Streamlit/Flask app (`app/main.py`)  
- [ ] Upload video â†’ see prediction  

---

## 9. Docs & Writeup
- [x] Quick Usage & Notes (`README.md`)  
- [x] Roadmap (`ROADMAP.md`)  
- [ ] Detailed report (later, for thesis)  

---

ðŸ“Œ **Current Status:**  
We have preprocessing and feature extraction (except context), plus a basic fusion model.  
Next milestone: **implement missing unimodal models (`visual_net.py`, `audio_net.py`, `context_net.py`)** before full training + prediction.
